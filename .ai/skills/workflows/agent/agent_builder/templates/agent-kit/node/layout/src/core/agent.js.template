import fs from 'fs';
import path from 'path';
import { loadConfig, log } from './config.js';
import { AgentError } from './errors.js';
import { createOpenAICompatibleClient } from './llm_openaiCompatible.js';
import { getToolSpecs, executeToolCall } from './toolRegistry.js';

function readPrompt(relPath) {
  const p = path.join(process.cwd(), relPath);
  return fs.readFileSync(p, 'utf8');
}

function safeJsonParse(str) {
  try { return JSON.parse(str); } catch { return null; }
}

function makeRequestId() {
  return 'req_' + Math.random().toString(16).slice(2);
}

export async function runAgent(runRequest, ctx = {}) {
  const cfg = loadConfig();
  const requestId = ctx.requestId || runRequest?.request_id || makeRequestId();

  if (!cfg.enabled) {
    throw new AgentError('AGENT_DISABLED', 'Agent is disabled (AGENT_ENABLED=false).', { requestId });
  }

  if (!cfg.llm.apiKey) {
    throw new AgentError('MISSING_LLM_API_KEY', 'LLM_API_KEY is not configured.', { requestId });
  }

  const client = createOpenAICompatibleClient({
    baseUrl: cfg.llm.baseUrl,
    apiKey: cfg.llm.apiKey,
    model: cfg.llm.model,
    timeoutMs: cfg.llm.timeoutMs,
  });

  const systemPrompt = readPrompt('prompts/base/system.md');
  const developerPrompt = readPrompt('prompts/base/developer.md');

  // Use a single system message for maximum compatibility with OpenAI-compatible providers.
  const combinedSystem = [
    systemPrompt.trim(),
    '',
    '---',
    '',
    developerPrompt.trim(),
  ].join('\n');

  const tools = getToolSpecs();
  const messages = [
    { role: 'system', content: combinedSystem },
    { role: 'user', content: JSON.stringify({ run_request: runRequest }, null, 2) }
  ];

  const maxSteps = cfg.limits.maxSteps;

  log('info', { msg: 'agent_run_start', requestId, agentId: cfg.agentId, model: cfg.llm.model });

  for (let step = 0; step < maxSteps; step++) {
    const resp = await client.chat({ messages, tools });

    const choice = resp?.choices?.[0];
    const msg = choice?.message;

    // Tool calls (OpenAI function calling)
    if (msg?.tool_calls && Array.isArray(msg.tool_calls) && msg.tool_calls.length > 0) {
      messages.push({ role: 'assistant', tool_calls: msg.tool_calls, content: msg.content ?? null });

      for (const tc of msg.tool_calls) {
        const name = tc?.function?.name;
        const args = safeJsonParse(tc?.function?.arguments || '{}') || {};
        const started = Date.now();

        let toolResult;
        try {
          toolResult = await executeToolCall(name, args, { requestId, cfg });
        } catch (e) {
          // Tool errors are returned as tool output; the model decides how to proceed.
          toolResult = { __tool_error__: true, error: { code: String(e.code || 'TOOL_ERROR'), message: String(e.message || e), details: e.details || {} } };
        }

        const durationMs = Date.now() - started;
        log('info', { msg: 'tool_call', requestId, tool: name, durationMs });

        messages.push({
          role: 'tool',
          tool_call_id: tc.id,
          content: JSON.stringify(toolResult)
        });
      }
      continue;
    }

    // Final answer path
    const content = msg?.content || '';
    const parsed = safeJsonParse(content);

    if (!parsed) {
      // Ask model to retry with strict JSON.
      messages.push({ role: 'assistant', content });
      messages.push({
        role: 'user',
        content: 'Your last message was not valid JSON. Reply again with ONLY valid JSON matching the RunResponse schema.'
      });
      continue;
    }

    log('info', { msg: 'agent_run_success', requestId, agentId: cfg.agentId });
    return parsed;
  }

  throw new AgentError('MAX_STEPS_EXCEEDED', `Exceeded maxSteps=${maxSteps} without producing a final JSON response.`, { requestId });
}
